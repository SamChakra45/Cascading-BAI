# -*- coding: utf-8 -*-
"""BAI-Simulations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1coyCMCevNUpUPPusl_67LnRvmJtrGath
"""

import numpy as np
import matplotlib.pyplot as plt

# =============================================================================
#  1. The Problem: Cascading Bandit Environment
# =============================================================================
# This class defines the rules and behavior of the cascading bandit problem.
# An algorithm will interact with this environment to learn the best arms.

class CascadingBanditEnvironment:
    """
    Simulates a cascading multi-armed bandit problem.

    This environment holds the secret "true" click probabilities for each arm
    and simulates the cascading user feedback model for a given list of arms.
    """
    def __init__(self, true_probabilities):
        """
        Initializes the environment.

        Args:
            true_probabilities (list or np.array): A list containing the true,
                unknown click probability for each arm.
        """
        self.true_probabilities = np.array(true_probabilities)
        self.L = len(true_probabilities)
        print(f"Environment initialized with {self.L} arms.")

    def simulate_round(self, selected_arms):
        """
        Simulates a single round of the cascading model.

        Args:
            selected_arms (list or np.array): An ordered list of arm indices
                chosen by the algorithm for this round.

        Returns:
            dict: A dictionary containing the feedback from the round:
                'observed_arms' (list): The arms the user actually saw.
                'clicked_arm' (int or None): The index of the arm that was
                                             clicked, or None if no click.
                'reward' (int): 1 if a click occurred, 0 otherwise.
        """
        observed_arms = []
        clicked_arm = None
        reward = 0

        for arm_index in selected_arms:
            observed_arms.append(arm_index)

            # Simulate a Bernoulli trial with the arm's true probability
            if np.random.rand() < self.true_probabilities[arm_index]:
                # A click occurred!
                clicked_arm = arm_index
                reward = 1
                break # The cascade stops here

        return {
            'observed_arms': observed_arms,
            'clicked_arm': clicked_arm,
            'reward': reward
        }

# =============================================================================
#  2. The Algorithms (Agents)
# =============================================================================

class CascadeUCB1Agent:
    """
    Implements the CascadeUCB1 algorithm (exploration + exploitation).
    """
    def __init__(self, L, K, alpha=1.5):
        self.L = L
        self.K = K
        self.alpha = alpha
        self.observation_counts = np.zeros(L)
        self.success_counts = np.zeros(L)
        self.t = 0

    def select_arms(self):
        self.t += 1
        ucb_values = np.full(self.L, np.inf)
        observed_mask = self.observation_counts > 0
        if np.any(observed_mask):
            empirical_means = self.success_counts[observed_mask] / self.observation_counts[observed_mask]
            exploration_bonus = np.sqrt((self.alpha * np.log(self.t)) / self.observation_counts[observed_mask])
            ucb_values[observed_mask] = empirical_means + exploration_bonus
        return np.argsort(ucb_values)[::-1][:self.K]

    def update(self, feedback):
        observed = feedback['observed_arms']
        clicked = feedback['clicked_arm']
        self.observation_counts[observed] += 1
        if clicked is not None:
            self.success_counts[clicked] += 1

class PureExplorationAgent:
    """
    Implements a pure exploration algorithm.

    This agent selects the K arms that have been observed the fewest number
    of times, effectively trying to give all arms equal attention.
    """
    def __init__(self, L, K):
        self.L = L
        self.K = K
        self.success_counts = np.zeros(L)
        self.observation_counts = np.zeros(L)

    def select_arms(self):
        return np.argsort(self.observation_counts)[:self.K]

    def update(self, feedback):
        observed = feedback['observed_arms']
        clicked = feedback['clicked_arm']
        self.observation_counts[observed] += 1
        if clicked is not None:
            self.success_counts[clicked] += 1

class SuccessiveEliminationAgent:
    """
    Implements a successive elimination algorithm based on phases.
    """
    def __init__(self, L, K, T):
        self.L = L
        self.K = K
        self.T = T

        # Total phases: P = log2(L - K)
        num_phases = int(np.floor(np.log2(L - K))) if (L - K) > 1 else 1
        self.num_phases = max(1, num_phases)
        self.rounds_per_phase = T // self.num_phases

        # Surviving items: S = [L]
        self.surviving_arms = list(range(L))
        self.current_phase = 1
        self.round_in_phase = 0

        # Phase-specific statistics
        self.phase_success_counts = np.zeros(L)
        self.phase_observation_counts = np.zeros(L)

    def _end_phase(self):
        """Handles logic for eliminating arms at the end of a phase."""
        if not self.surviving_arms: return

        # Calculate empirical means for surviving arms
        means = {}
        for arm_idx in self.surviving_arms:
            if self.phase_observation_counts[arm_idx] > 0:
                means[arm_idx] = self.phase_success_counts[arm_idx] / self.phase_observation_counts[arm_idx]
            else:
                means[arm_idx] = 0

        # Sort survivors by their estimated mean
        sorted_survivors = sorted(self.surviving_arms, key=lambda arm: means[arm], reverse=True)

        # Remove L-K / 2^(p+1) arms with smallest estimates
        num_to_remove = int(np.floor((self.L - self.K) / (2**(self.current_phase + 1))))

        # Ensure we don't remove too many arms (at least K must survive)
        num_to_remove = min(num_to_remove, len(self.surviving_arms) - self.K)

        if num_to_remove > 0:
            self.surviving_arms = sorted_survivors[:-num_to_remove]

        # Reset for the next phase
        self.current_phase += 1
        self.round_in_phase = 0
        self.phase_success_counts.fill(0)
        self.phase_observation_counts.fill(0)

    def select_arms(self):
        # Check if the current phase is over and we are not in the last phase
        if self.round_in_phase >= self.rounds_per_phase and self.current_phase < self.num_phases:
            self._end_phase()

        # Select K items from survivors with the smallest number of observations in this phase
        # Create a mask for surviving arms to easily filter counts
        survivor_counts = {arm: self.phase_observation_counts[arm] for arm in self.surviving_arms}
        sorted_by_count = sorted(survivor_counts, key=survivor_counts.get)

        return sorted_by_count[:self.K]

    def update(self, feedback):
        self.round_in_phase += 1
        observed = feedback['observed_arms']
        clicked = feedback['clicked_arm']

        # Update phase statistics
        self.phase_observation_counts[observed] += 1
        if clicked is not None:
            self.phase_success_counts[clicked] += 1

# =============================================================================
#  3. Simulation Runner
# =============================================================================

def run_simulation(environment, agent, T):
    """
    Runs a simulation for a given agent and environment.
    """
    reward_history = []
    # Re-initialize agent state for a clean run
    agent.__init__(agent.L, agent.K, T) if isinstance(agent, SuccessiveEliminationAgent) else agent.__init__(agent.L, agent.K)

    for _ in range(T):
        selected_arms = agent.select_arms()
        feedback = environment.simulate_round(selected_arms)
        agent.update(feedback)
        reward_history.append(feedback['reward'])
    return np.array(reward_history)


# =============================================================================
#  4. Main Execution and Comparison
# =============================================================================

if __name__ == '__main__':
    # --- Simulation Parameters ---
    L = 500    # Total number of arms
    K = 5       # Number of arms to select per round
    T = 10000   # Total number of rounds (budget)

    # --- Setup ---
    # Define the true probabilities (unknown to the agents)
    true_probabilities = np.sort(np.random.uniform(0.05, 0.4, L))[::-1]

    # 1. Initialize the Environment (The Problem)
    env = CascadingBanditEnvironment(true_probabilities)

    # 2. Initialize the Agents (The Algorithms)
    # ucb_agent = CascadeUCB1Agent(L=L, K=K)
    explore_agent = PureExplorationAgent(L=L, K=K)
    se_agent = SuccessiveEliminationAgent(L=L, K=K, T=T)

    # --- Run Simulations for All Agents ---
    # print("\nRunning CascadeUCB1 Agent...")
    # ucb_rewards = run_simulation(env, ucb_agent, T)

    print("Running Pure Exploration Agent...")
    explore_rewards = run_simulation(env, explore_agent, T)

    print("Running Successive Elimination Agent...")
    se_rewards = run_simulation(env, se_agent, T)

    # --- Print Final Results ---
    print("\n--- Simulation Finished ---")
    # print(f"CascadeUCB1 Total Reward:              {ucb_rewards.sum()} / {T} rounds ({100 * ucb_rewards.sum() / T:.2f}%)")
    print(f"Pure Exploration Total Reward:         {explore_rewards.sum()} / {T} rounds ({100 * explore_rewards.sum() / T:.2f}%)")
    print(f"Successive Elimination Total Reward:   {se_rewards.sum()} / {T} rounds ({100 * se_rewards.sum() / T:.2f}%)")

    # --- Plotting Comparison ---
    plt.figure(figsize=(12, 7))
    # plt.plot(np.cumsum(ucb_rewards), label='CascadeUCB1 (Explore/Exploit)')
    plt.plot(np.cumsum(explore_rewards), label='Pure Exploration')
    plt.plot(np.cumsum(se_rewards), label='Successive Elimination')
    plt.title('Algorithm Performance Comparison')
    plt.xlabel('Round (t)')
    plt.ylabel('Total Cumulative Clicks')
    plt.legend()
    plt.grid(True, linestyle=':')
    plt.show()